# 성형회귀 
- 종속변수(예측하는)와 하나 이상의 독립변수(예측하기 위한) 간의 선형관계를 모델링 하는 방법
- 독립변수의 수에 따라 단순 선형회귀와 다중 선형회귀로 나뉜다

- 단순선형회귀 = 하나의 독립 변수
- 다중선형회귀 = 여러 독립 변수
- ml에는 가중치가 있고 가중치에 따라 예측값이 결정 된다 예측값과 실제값을 비교하여 오차를 찯을 수 있고 오차를 줄어들게 하는게 학습이다

## 다항 회귀 
**다항 회귀(Polynomial Regression)는 종속 변수와 독립 변수 간의 비선형 관계를 모델링하는 방법**
- 다항회귀 차수(degree) : 독립 변수의 최대 차수
- 차수가 높을수록 모델이 더 복잡해지며 과적합(overfitting)의 위험 존재 → 적절한 차수 선택 필요

## 로지스틱 회귀(분류모델)
- 종속 변수가 이진형일떄 사용되는 통계 기법
- 결과값이 0과 1 사이에 위치하기 위해 시그모이드(Sigmoid Function) 함수 사용
- 로지스틱 회귀는 회귀 분석이지만, 실제로는 분류작업에 사용됩니다!!


# svm
- 서포트 벡터 머신(SVM)은 분류와 회귀 분석에 사용되는 강력한 지도학습 모델
- 데이터를 분류하기 위해 결정 경계(결정 초평면, hyperplane)를 찾아 분류합니다.
- 초평면은 두 클래스 사이의 최대 마진을 보장하는 방식으로 선택합니다.

- 마진 : 두 클래스 간의 가장 가까운 데이터 포인트 사이의 거리
- 서포트 벡터 : 결정 초평면에 가장 가까이 위치한 데이터 포인트 - 결정 초평면을 정의합니다.
- 커널 함수 : 데이터를 더 높은 차원으로 매핑하여 선형적으로 분리 할 수 없는 데이터를 분리하게 합니다. 

# knn(K-Nearest Neighbors)
- KNN 알고리즘은 분류와 회귀 분석에 사용되는 비모수적 방법 입니다.
- 새로운 데이터 포인트를 기존 데이터 포인트 중 가장 가까운 K개의 이웃과 비교하여 분류합니다
- 데이터 포인트의 특성을 기준으로 거리 계산을 통해 가장 가까운 이웃을 찾습니다.

## KNN의 목적

- KNN의 목표는 학습 데이터를 기반으로 새로운 데이터 포인트의 클래스를 예측하는 것입니다
- 이는 분류 문제에서 주로 사용되며 다양한 응용 분야에 활용될 수 있습니다

# 나이브베이즈란?

- 나이브베이즈(Naive Bayes) 분류기는 베이즈 정리를 기반으로 하는 통계적 분류 기법입니다.
- 나이브라는 이름이 붙은 이유는 각 특징(feature)이 독립적이라고 가정하기 때문입니다.
- 주로 텍스트 분류 문제에서 널리 사용됩니다.

## 나이브베이즈의 종류

- 가우시안 나이브베이즈: 특징들이 연속적이고 정규 분포를 따른다고 가정합니다.
- 베르누이 나이브베이즈: 특징들이 이진수(0 또는 1)로 표현되는 경우 사용합니다.
- 멀티노미얼 나이브베이즈: 특징들이 다항 분포를 따르는 경우 사용합니다.

## 나이브베이즈의 목적

- 나이브베이즈의 목표는 주어진 데이터 포인트가 특정 클래스에 속할 확률을 계산하여 분류하는 것입니다.
- 이 모델은 단순하고 계산이 효율적이며, 텍스트 분류와 같은 문제에서 좋은 성능을 발휘합니다

# 의사결정 나무
- 의사결정나무(Decision Tree)는 예측 모델 중 하나로, 데이터의 특징(feature)을 기준으로 의사결정 규칙을 만들고 이를 바탕으로 데이터를 분류하거나 회귀하는 데 사용됩니다
- 의사결정나무는 트리 구조를 가지며, 각 내부 노드는 데이터의 특정 특징에 대한 테스트를 나타내고, 각 가지(branch)는 테스트 결과를 나타내며, 각 리프 노드(leaf)는 클래스 레이블을 나타냅니다.
- 의사결정나무는 불확실성이 낮아지도록 규칙을 만들어라

## 용어
- 노드: 트리의 각 분기점으로, 하나의 특징(feature)에 대한 테스트를 나타냅니다.
- 루트 노드: 트리의 최상위 노드로, 전체 데이터셋을 나타냅니다.
- 리프 노드: 트리의 끝 노드로, 최종 클래스 레이블을 나타냅니다.
- 깊이: 트리의 루트 노드부터 리프 노드까지의 최대 거리입니다.
- 분할 기준: 노드를 나눌 때 사용하는 기준으로, 정보 이득(Information Gain), 지니 계수(Gini Index) 등이 있습니다.

## 분할기준

- 정보 이득(Information Gain) : 엔트로피(Entropy)값으로 데이터를 나누는 기준입니다. 엔트로피는 불확실성을 나타내며, 엔트로피가 낮을수록 불확실성이 적습니다.
- 지니 계수(Gini Index): 불순도를 측정하는 방법으로, 지니 계수가 낮을수록 불순도가 적습니다